\documentclass[xetex,svgnames]{scrartcl}

% packages
\usepackage{xltxtra}
\usepackage{polyglossia}
\usepackage[
    urlcolor=black,
    linkcolor=black,
    citecolor=black,
    filecolor=black,
    pagecolor=black,
    linktocpage=true,
    ]{hyperref}
\usepackage{fontspec}
\usepackage{scrpage2}
\usepackage[left=2cm,right=2cm,top=3cm,bottom=3cm]{geometry}
\usepackage{colortbl,color,xcolor}
\usepackage{alltt}
\usepackage{listings}
\usepackage{multirow}
\usepackage{csquotes}

% fonts general
\setmainfont[Mapping=tex-text,Scale=1.0]{FreeSans}
\setsansfont[Mapping=tex-text,Scale=1.0]{FreeSans}
\setmonofont{FreeMono}

% special fonts
\newfontfamily\hana{HAN NOM A}
\newfontfamily\hanb{HAN NOM B}
\newfontfamily\sil{Charis SIL}





\newcommand{\White}[1]{\cellcolor{white} \textcolor{black}{ #1}}

% language settings
\setmainlanguage{english}
\setotherlanguage{german}

% pagestyle settings
\pagestyle{scrheadings}
\ihead{M.-S. Wu and J.-M. List}
\chead{Computer-Assisted Language Comparison}
\ohead{+++DATE+++}
\ifoot{}
\cfoot{\pagemark}
\ofoot{}

\usepackage[
    alldates=terse,
    backend=bibtex,
    %backref=true,
    bibstyle=authoryear,
    firstinits=true,
    ibidtracker=strict,
    isbn=false,doi=false,url=false,
    labelnumber=true,
    loccittracker=strict,
    hyperref=false,
    maxbibnames=10,
    maxcitenames=2,
    natbib=true,
    opcittracker=strict,
    sortcites=true,
    sorting=nyt,
    defernumbers=true,
    style=authoryear-ibid,
    terseinits=false
    ]{biblatex}

\input{bibstyle}
\newcommand{\Table}[1]{%
    \begin{flushleft}
        \begin{tabular}{|p{14.5cm}|}
            \hline \cellcolor{lightgray} \bf \pur #1
            \\\hline
        \end{tabular}
    \end{flushleft}
}

\bibliography{evobib.bib}

\usepackage{zhspacing}

\begin{document}
\zhspacing
%\maketitle

\begin{center}
    {\bfseries \huge  Computer-Assisted Language Comparison: State of the Art}
\end{center}

\begin{abstract}
  \small
  By comparing the languages of the world, we gain invaluable insights into human prehistory,
  predating the appearance of written records by thousands of years. The traditional methods for
  language comparison are based on manual data inspection. With more and more data available, they
  reach their practical limits. Computer applications, however, are not capable of replacing
  experts’ experience and intuition. In a situation where computers cannot replace experts and
  experts do not have enough time to analyse the massive amounts of data, a new framework, neither
  completely computer-driven, nor ignorant of the help computers provide, becomes urgent. Such
  frameworks are well-established in biology and translation, where computational tools cannot
  provide the accuracy needed to arrive at convincing results, but do assist humans to digest large
  data sets. In this talk, we will illustrate what we consider the current state of the art of
  computer-assisted language comparison, by presenting a workflow that starts from raw data and
  leads up to a stage where sound correspondence patterns across multiple languages have been
  identified and can be readily presented, inspected, and discussed. We illustrate this workflow
  with help of a dataset on Hmong-Mien languages, which has so far not yet been analyzed in this
  way. Our illustration is furthermore accompanied by Python code and instructions on how to make
  use of additional web-based tools we developed, so that users can replicate our workflow or apply
  it for their own purposes.

\end{abstract}

\section{Introduction}
\subsection{The Gap between Computational and Traditional Historical Linguistics}

The proposal of new, fancy, and shiny quantitative methods applied to handle problems in historical
linguistics has created a gap between what one could call ``classical" approaches to historical
language comparison and the ``new and innovative" automatic approaches.
Classical linguists are often skeptical of the new approaches, partly because the results differ from
those achieved by classical methods \citep{Anthony2015,Holm2007}, but also because the majority of the
new approaches work in a black box fashion and do not allow inspecting the concrete findings in
detail. Computational linguists, on the other hand, complain about classical historical linguists' lack
of consistency when applying the classical methods.

\subsection{Computer-Assisted Disciplines}

The use of computer applications in historical linguistics is steadily increasing. With more and
more data available, the classical methods reach their practical limits. At the same time, computer
applications are not capable of replacing experts' experience and intuition, especially when data
are sparse. If computers cannot replace experts and experts do not have enough time to analyse the
massive amounts of data, a new framework is needed, neither completely computer-driven, nor ignorant
of the assistance computers afford. Such computer-\emph{assisted} frameworks are well-established in
biology and translation. Current machine translation systems, for example, are efficient and
consistent, but they are by no means accurate, and no one would use them in place of a trained
expert. Trained experts, on the other hand, do not necessarily work consistently and efficiently. In
order to enhance both the quality of machine translation and the efficiency and consistency of human
translation, a new paradigm of computer-assisted translation has emerged \citep[3]{Barrachina2008}.

\subsection{Computer-Assisted Language Comparison}

Following the idea of computer-assisted frameworks in translation and biology, a framework for
computer-assisted language comparison (CALC) could be the key to reconcile classical and
computational approaches in historical linguistics. Computational approaches may still not be able
to compete with human experts, but when used to pre-process the data with human experts
systematically correcting the results, they can drastically increase both the efficiency and the
consistency of the classical comparative method.


\begin{figure}[htb]
  \centering
  \includegraphics[width=0.75\textwidth]{calc-figure-1.png}
  \caption{Basic idea of data managment within the CALC framework.}
  \label{fig:calc}
\end{figure}

The basic idea behind computer-\emph{assisted} as opposed to computer-\emph{based} language
comparison is to allow scholars to do qualitative and
quantitative research are done at the same time. In order to allow scholars to do this, \textbf{data
must always be available in \emph{machine-} and \emph{human-readable} form}.
Figure \ref{fig:calc} shows a tentative workflow for the CALC framework, in which data is constantly passed back and
forth between computational and classical linguists.

Three different aspects are essential for this workflow:
\begin{itemize}
  \item[(a)] New
software allows for the application of transparent methods which increase the accuracy and the application range
of current methods and also treat the peculiarities of specific language families (like, e.g.,
Sino-Tibetan).
\item[(b)] Interactive tools provide
an interface between human and machine, allowing experts to correct errors and to inspect the automatically
produced results in detail.
\item[(c)] Specific data is used to test and train the software algorithms.
  \end{itemize}



\section{Workflows for Computer-Assisted Language Comparison}

\subsection{Overview}
Our workflows for computer-assisted language comparison have so far been intensively tested on a
small set of 8 Burmish languages, which we investigated in collaboration with Nathan W. Hill, who
was responsible for the qualitative investigation of the data and for the common discussion of new
computer-assisted methods which were then implemented by Johann-Mattis List (see \citealt{Hill2017a}
for an exemplary discussion of some of the new approaches). Our experience with the
Burmish project by now allows us to set up a first workflow that starts from raw data and leads up
to the explicit identification of correspondence patterns across multiple languages. At the moment,
List and Hill develop the workflow further to account also for (semi)-automatic reconstructions, but
in this talk, only the identification of correspondence patterns will be discussed.

\subsection{Details of the Workflow}

Our workflow currently comprises 5 different stages, in which we successively lift linguistic data
from their raw form in which we can find them in wordlists and tables published in dictionaries and
field-work notes, up to a level where correspondence patterns across cognate words have been
automatically identified and can be qualitatively inspected by the scholar.

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.8\textwidth]{calc-workflow.pdf}
  \caption{Current state-of-the-art workflow developed in collaboration of different research groups
  working in computer-assisted frameworks.}
  \label{fig:workflow}
\end{figure}

Although the workflow can be carried out almost completely without any manual intervention by a
linguist, we emphasize that this workflow explicitly \emph{allows} for expert intervention at
\emph{any} of the five stages. While, in our experience, specific care is required when lifting the
data the first time to machine-readable format, it should further be noted that \emph{all} steps of
the workflow profit from human intervention, since none of the automatic methods currently available
to us could spot all patterns in linguistic data without over- or underestimating their importance
for linguistic reconstruction.

Our workflow starts from \emph{raw data}, including tabular data from fieldwork notes or data
published in books and articles, which we re-organize and re-format in such a way that the data can
be processed by our tools. Once we have \emph{machine-readable data}, we can use methods for
automatic cognate detection \citep{List2016g} in order to infer \emph{partial cognates} across the
languages in our data. Having inferred cognates, we can now also align the data in the cognate sets.
While we could use phonetic alignment approaches discussed in the literature \citep{List2014d}, we
now use a new approach, based on phonotactic templates, which has the advantage of being much
faster and accurate when dealing with alignments for South-East-Asian languages. Once having
identified the alignments, we start to search automatically for cognates \emph{across} different
concepts. Since all automatic methods \emph{need} to start searching for cognates within the same
concept slot (otherwise, there would be too many false positives), our new method, which makes used
of a systematic comparison of readily aligned cognate sets, systematically searches for cognates
independent of their meaning. The improved, cross-semantic cognate sets, which are all readily
aligned, have the specific property of being \emph{strict}: no cognate set could compare two
morphemes from the same language which would differ in their pronunciation. \citep{List2018PBLOG7}
calls these cognate sets \emph{regular}, but in discussions with Nathan Hill, we decided that
\emph{regular} is probably not the best term, as they can well be wrong, so we call them
\emph{strict} now. Once strict cognates have been identified, we use the new algorithm for the
automatic inference of sound correspondence patterns across multiple languages by \citet{List2019a}
to infer the correspondence patterns in the data.

In Section \ref{sec:wf}, we will provide detailed examples how all steps of the workflow interact,
using a relatively recent collection of linguistic data on Hmong-Mien languages \citep{Chen2012} for
this purpose.
\subsection{Materials and Methods for the Workflow Illustration}

The data we use to illustrate our workflow in the next section was originally collected by
\citet{Chen2012}, and later added in digital form to the Wiktionary project. Chén's collection of
\emph{frequent terms} (\emph{chángyòng cíbiǎo 常用词表}, pp. 567-862) comprises 885 different
concepts translated into 25 varieties of Hmong-Mien. In Figure \ref{fig:data}, we contrast one
exemplary page from Chéns book with the data as it has been prepared by the Wiktionary users.
We can see that the data is essentially the same, but that the rows and columns of the tabular form
have been swapped.

\begin{figure}[htb]
  \centering
  \includegraphics[width=\textwidth]{chen-illustration.pdf}
  \caption{Contrasting Chén's original data with the table in Wiktionary}
  \label{fig:data}
\end{figure}

All methods have either been implemented and published before, or are shared along with the slides
and the handout for this talk. Since this is work in progress, however, we warn users that both data
and code will be in flux for some time, but we will make sure that both data and code can always be
readily analyzed with our tools. All code, the data we use, and installation instructions can be
found at \url{https://github.com/lingpy/calc-workflow}. We ask those interested in testing our
methods to use our issue-tracker on GitHub in case they face difficulties of any kind.
In this talk, we present the workflow with a subset of 10 varieties of the Hmong-Mien languages in
Chén's sample, for which we selected a subset of 313 concepts. The concepts were selected by
checking the overlap with the current 504 concept list of the Burmish Etymological Database project
(headed by Nathan W. Hill, data online at \url{https://dighl.github.io/burmish}). 
The languages were selected for some general reasons, like lexical coverage, geographic
distribution, or basic diversity, but not with the specific ``eye" of a historical linguist who
would select languages to explore the history of a language family. We would be glad about any additional
recommendations, if scholars feel competent to give us advice in this context.
The
geographic locations are shown in the Figure \ref{fig:geo}.

\begin{figure}[htb]
  \centering
  \includegraphics[width=\textwidth]{Geographic.pdf}
  \caption{Language geographic locations}
  \label{fig:geo}
\end{figure}

\section{Illustration of the Workflow}\label{sec:wf}
\subsection{From Raw Data to Segmented Data}
When comparing languages within a computer-assisted framework, with the goal of identifying sound
correspondence patterns in the data, we need to make sure that our data is machine-readable at
first. If the data is not machine-readable, we can neither use web-based tools like EDICTOR which
make it easy to edit the data \emph{manually} \citep{List2017d}, nor can we use computational tools,
like LingPy \citep{List2018i}, which can help us a great deal in identifying cognate sets and
aligning our data.

A first problem for many researchers is to get used to our formats for data representation. 
In contrast to the typical style used by scholars, we do not use simple tables, with languages in a
row and concepts in a column, or vice versa, but instead a so-called long-table format, in which we
reserve a \emph{row} in a table for each word, and add a er, which tells us what the cells in
each column contain in terms of the data. This long-table format reflects the rule of
``One Value per Cell'', as stated by the Cross-Linguistic Data Formats initiative
\citep{Forkel2018a}, reproduced in Figure \ref{fig:onevalue}. 

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.75\textwidth]{one-value-per-cell.png}
  \caption{Long-table format instead of condensed formats with multiple values per cell.}
  \label{fig:onevalue}
\end{figure}

As a second rule, we have certain format specifications that make it easier form machines to deal
with our input. This includes

\begin{itemize}
  \item the use of a \emph{segmented} form of IPA transcriptions, in which a space is used to
    separate distinct sounds from each other, to give the computer direct information on whether
    symbol combinations are meant to reflect one sound (e.g., affricates, such as {\sil [ts, tʃ]}),
    or multiple sounds (compare German \emph{Handschuh} {\sil [h a n t ʃ uː]} vs. German
    \emph{Tschüss} {\sil [tʃ y s]}),
  \item them use of morpheme segmentation markers (we use a \texttt{+}) to indicate morpheme
    boundaries, which is straightforward when working with many morpheme-syllabic SEA languages, in
    which morphemes coincide with syllables,
  \item a clear-cut account on the concepts in our data, as they serve as the initial comparanda, so
    each concept needs to be given a clear-cut definition, and our preferable starting points are
    concept lists which are translated into the languages to be investigated, as opposed to
    pre-selected accounts on potential etymological items.

\end{itemize}

We indicate words in the computer-readable form, by adding a column called \texttt{TOKENS} in which
data is segmented with a space to distinguish different sounds, and with the plus-symbol to
distinguish different morphemes. 

Thus, our original data consists of a text-file, separated by tabstop, with the first row serving as
a header, and the following rows providing information for one word per language. Our software
requires the following columns to be submitted:

\begin{itemize}
  \item \texttt{ID}: numerical identifier, greater than 0,
  \item \texttt{DOCULECT}: name of the language,
  \item \texttt{CONCEPT}: some gloss for the concept,
  %\item VALUE: original form of the data,
  %\item FORM: the form in the data after correcting errors, or multiple forms-per-cell problems,
  \item \texttt{TOKENS}: the morpheme and sound-segmented form of the data.
\end{itemize}

We recommend also to add a column called \texttt{VALUE}, containing the original data, as well as a
column \texttt{FORM}, which shows the original data but corrected for multiple values per cell. The
software usually automatically creates a form \texttt{IPA}, which is not necessarily used, but a
legacy form that will be replaced by the \texttt{FORM} in future updates.
Additional values are then consistently added by our workflow and will be discussed later.
 
We offer procedures to ease the conversion of the data to the required formats. While the creation
of long-table formats is usually done by applying a custom script, we use \emph{orthography
profiles} to create morpheme-segmented IPA representations for our \texttt{TOKENS} column from the
original data \citep{Moran2018}. Orthography profiles are a very straightforward way to convert raw
data to space-separated IPA representations. An orthography profile can be thought of as a simple text file with two or more columns in which the first
represents the values as you find them in your data (i.e., non-IPA transcriptions, etc.), and the other
columns allowing you to convert the sequence of characters that you find in the first column. So in brief,
you have a source-pattern and a replacement pattern, for example, the one shown in Table
\ref{tab:profile}. With such a replacement pattern, an input string \texttt{čashaa} would on the one
hand be segmented into \texttt{č a sh aa} and at the same time, it would be converted to \texttt{ tʃ
a ʃ aː}. We now offer an online demo of orthography profiles at
\url{http://calc.digling.org/profile}, which can be used to test and apply customized orthography
profiles.

\begin{table}[htb]
  \centering
  \tabular{ll}
  \textbf{Grapheme} & \textbf{IPA} \\
  č &  tʃ \\
  ž & dʒ\\
  th&  tʰ\\
  dh&  d̤\\
  sh&  ʃ\\
  a & a\\
  aa&  aː\\
\endtabular
\caption{Very simple orthography profile example.}
\label{tab:profile}
\end{table}

\begin{center}
  \tabular{|p{14cm}|}\hline
  SUMMARY \\\hline
  \begin{itemize}
    \item Data must be machine-readable in order to be amenable for computer-assisted analyses.
    \item Data must specifically be segmented, both with respect to the morpheme boundaries and the
      boundaries between distinct sounds.
    \item Data must be provided in form of a \emph{long table} with some specific column headers,
      providing all relevant information.
    \item Computer-assisted tools help to prepare the data for computer-assisted processing.
  \end{itemize}\\\hline
  \endtabular
\end{center}

\subsection{From Segmented Data to Cognate Sets}\label{sec:pcogs}

Once the data is segmented and provided in the long table format as it is required by the LingPy
software package, as described in our tutorial \citep{List2018f}, we can use LingPy's partial
cognate detection method to infer partial cognates in our linguistic data. Partial cognates are
hereby understood as cognate assessments \emph{per morpheme} in our data, as opposed to cognate
assessments \emph{per word}. 
While it has always been clear to scholars working in the field of South-East Asian linguistics that
cognacy should rather be assigned on the level of the morpheme than on the level of full words,
given that the high degree of compounding would easily complicate the identification of cognate
relations, automatic methods, and specifically phylogenetic reconstruction approaches usually still
assume a rather naive one-word-one-cognate relation \citep{List2016f}. 

In our framework, we explicitly address this problem by adopting a numerical annotation format in
which each morpheme instead of each word form is assigned to a specific cognate set
\citep{Hill2017a}. This framework is illustrated in Figure \ref{fig:pcogs}, where we contrast word
forms for ``yesterday" in five Burmish varieties, indicating their detailed ``cognate relations". 
In the first ``traditional" style of cognate coding, we would proceed in a \emph{strict} way, only
allowing those words which are completely cognate in all their morphemes to be judged as cognates.
In the second, \emph{loose} cognate annotation, we judge all words that are in a \emph{connected
component} in our shared morpheme network to be cognate, and in the last column, we show our
explicit coding of partial cognacy, in which each morpheme is assigned to one cognate set.

\begin{figure}[htb]
  \centering
  \tabular{cc}
  \includegraphics[width=0.45\textwidth]{pcogs-1.png} &
  \includegraphics[width=0.45\textwidth]{pcogs-2.png} \\\endtabular
  \caption{Partial cognacy in Burmish language varieties and different ways of coding (see Hill and
  List 2017 and further explanations in the main text).
  coding.}
  \label{fig:pcogs}
\end{figure}

The software package LingPy offers a straightforward algorithm to detect and annotate partial
cognates in datasets formatted as long tables. This algorithm by \citet{List2016g} uses techniques for automatic sequence
comparison to create a network of similar morphemes for each meaning slot in a given dataset. It
then filters those concepts in consecutive stages, with the goal of avoiding that two or more
morphemes in the same word for the same language are assigned to the same cluster. In the end, the
algorithm outputs the cognate judgments in the same format as indicated above in Figure
\ref{fig:pcogs}, namely, but assigning each morpheme to a given number, with the number representing
that cognate set.

Note that this algorithm works quite well, although it is, of course, not infallible. It reaches
between 88 and 90 percent on a test datasets consisting of Bai dialects, Chinese dialects, and
dialects of Tujia. With more challenging datasets, the scores will surely drop, but we can expect
that the automatic cognate detection is in any case \emph{helpful}, as is easier to correct cognates
than to assign them from scratch.

In addition to the cognate detection algorithm, the EDICTOR web-based tool for computer-assisted
language comparison \citep{List2017d}, freely available at \url{http://edictor.digling.org}, can be
used to quickly inspect and correct computer-generated cognate sets, by providing a very convenient
interface that allows users to quickly assign morphemes to cognate sets. The interface is
illustrated in Figure \ref{fig:edipart}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{partial-cognates.png}
  \caption{Partial cognate annotation within the EDICTOR tool for the word for ``chin" in 10
  selected Hmong-Mien varieties.}
  \label{fig:edipart}
\end{figure}

\begin{center}
  \tabular{|p{14cm}|}\hline
  SUMMARY \\\hline
  \begin{itemize}
    \item For a realistic annotation of cognate sets, the annotation of partial cognates, by which
      morphemes are assigned to cognate sets, is the only realistic choice.
    \item Partial cognates can be automatically identified with help of software, openly available
      as part of the LingPy software library (\url{lingpy.org}, \citealt{List2018i}) and the
      algorithm by \citet{List2016g}.
    \item Partial cognates can be annotated consistently with help of the EDICTOR tool
      \citep{List2017d}, online available at \url{http://edictor.digling.org}.
    \item Partial cognates in these frameworks are assigned to morphemes occurring in words with the
      same meaning, both for algorithmic and for practical reasons.
  \end{itemize}\\\hline
  \endtabular
\end{center}

\subsection{From Cognate Sets to Alignments}
Wang et al pointed out that Sinitic languages follows a strict syllable strcuture as in the form of ~\citep{Wang1996}, 

\begin{displayquote}
syllable = (Tone (Initial (Medial (Nucleus + ending))))  
\end{displayquote}

As many SEA languages seem to have similar syllable structures as Sinitic languages, this phonotactic template can help us aligning the segmentations as well. For this task, we need a clear guideline to align the segmentations. For LingPy to align segmentations according to the template we set up, an orthography profile comes in handy for this aspect, to indicate the alignment rules, we use i, m, n, c and t to represent Initial, Meidial, Nucleus, Coda (ending) and Tone in the profile. 

\begin{table}
\begin{tabular}{cccc}
Grapheme & Segments & IPA & Structure \\
tʰ & tʰ & tʰ & i \\
v & v & v & i \\
ɛ & ɛ & ɛ & n \\
au & au & au & n\\
jei & j ei & j ei & m n \\
oŋ & o ŋ & o ŋ & n c \\
wɑŋ & w ɑ ŋ & w ɑ ŋ & m n c \\
ȵoŋ & ȵ o ŋ & ȵ o ŋ & m n c \\
³¹ & ³¹	& ³¹ & t \\
\end{tabular}
\label{An example of orthography profile for template alignment.}
\end{table}

And the alignment results can be view on EDICTOR as the picture below.
\begin{figure}[htb]
\includegraphics[width=0.45\textwidth]{template-alignment.png}
\end{figure} 

An important message is that, the template can be flexibly adjusted by linguistic experts, for example, one can use C to represent consonants and V as vowels and set up a template as CV, VC, CVCC or other varieties. 

\subsection{From Alignments to Cross-Semantic Cognates}
As mentioned above in Section \ref{sec:pcogs}, the partial cognates are only identified for words
with the same meaning. This is being done for algorithmic reasons (it would become quite complex to
compare all morphemes against each other algorithmically), and for practical reasons, since we
believe that it is always better to start from the obvious and save etymologies in historical
linguistics, rather than to start from complex ones. Given that semantic shift is a phenomenon for
which we dispose of little knowledge with respect to its patterns, we agree explicitly with scholars
like \citet{Dybo2008} in emphasizing that we should always expect to find clear-cut etymologies
within words of the same meaning, even if we know that more etymologies could be find when searching
\emph{cross-semantically}, i.e., among words which differ with respect to their meanings.
 
There are only a few approaches that try to identify cognates across different concepts, and one
could say that the task of \emph{cross-semantic cognate detection} is still one of the open problems
in computational historical linguistics. Approaches proposed so far include a rather complex
workflow by \citet{Wahle2016}, who uses \emph{hidden Markov models} for sequence comparison, and
proxies on colexifications, drawn from the database by \citet{Dellert2017}, to infer cognates across
different meaning slots. As this task is not completely evaluated, and only described in a short
paper, it is difficult to access its usefulness for our purposes. Another approach is presented by
\citet{Arnaud2017}, who apply Support Vector Machines trained on form and semantic similarities of
word pairs along with a flat clustering algorithm to partition words into cognate sets. 
While this approach is publicly available and seems to yield promising results, we are not sure to
which degree it would help us with our very specific goals of lifting an initially ``raw" dataset
to a level where we can assess sound correspondence patterns across multiple languages, especially
since the algorithms the authors use for cognate detection do \emph{not} take regular sound
correspondences into account, and they are also \emph{not} sensitive to partial cognates.
 
Thus, instead of these previously proposed solutions, we propose our own, rather simple approach to
search for cross-semantic partial cognate sets in our data. This approach is based on the
well-observed fact that the majority of morphemes in South-East Asian languages with a certain
preference for compounding and a high degree of word formation, is highly \emph{promiscuous}
\citep[8f]{List2016h}, given
that they recur within different words, surfacing in the form of \emph{partial colexifications}
\citep[62]{Hill2017a}. The term \emph{partial colexification} hereby serves as a cover term for
morphemes recurring across the lexicon of a language, with no specific distinction being made if
they are polysemous or homophonous. 
 
Our search for partial colexifications would not allow us directly to identify cross-semantic
cognates consistently, given that sound change may yield different morpheme mergers across different
languages. As a result, we cannot take the information from one language alone, but have to smartly
summarize all the information on recurring morphemes we can find in our data. The solution for this
problem is nevertheless straightforward, and it builds on the idea to not only compare single words, as originally proposed in \citet{Hill2017a}, but to compare complete \emph{alignments} instead. As our data is already aligned, and we have identified
cognates in a first run, potentially even refined by experts, we can compare whole cognate sets that
contain \emph{identical words in the same language}. 

If two alignments are completely identical with respect to the words they contain, 
there is no reason
to assign them to different cognate sets, and we can directly assign them to the same cognate class.
Even if they are simply homophonous, the assumption of regular sound change will allow us to treat
them similarly if we reconstruct the words back to the ancestral language.

The problematic cases are those cases, where we have \emph{incomplete data}. And this is usually
rather the rule than the exception.  We often will encounter cases where we have two alignments
which are only filled in parts with data from the different languages, and we will usually have
\emph{missing data} for one or more of the languages in our sample in a given alignment. Thus, when
comparing two alignments with each other, we need to make sure that we have at least one word in one
language in common. 

As an example, consider the data on ``son'' and ``daughter'' in five language varieties of our
illustration data. As can be seen immediately, two languages show striking \emph{partial
colexifications} for the two concepts, Chuanqiandian and East Qiandong. In both cases, one morpheme
recurs in the words for the two concepts. In the other cases, we find different words, but if we
compare the overall cognacy, we can also see that all five languages share one cognate morpheme for
``son'' (corresponding to the Proto-Hmong-Mien {\sil *tu̯ɛn} in Ratliff's reconstruction), 
and three varieties share one cognate morpheme for ``daughter'' (corresponding to {\sil
*mphje\textsuperscript{D}} in Ratliff's 2010 reconstruction), with the morpheme for ``son''
occurring also in the words for ``daughter'' in East Qiandong and Chuanqiandian, as mentioned
before.\nocite{Ratliff2010}


\begin{table}[htb]
  \centering
  \resizebox{\textwidth}{!}{%
  \tabular{lllll}\hline
  \bfseries Language &
  \bfseries Concept &
  \bfseries Form &
  \bfseries Cognacy &
  \bfseries Cross-Semantic \\\hline\hline
  East Baheng                     & SON      & \sil \colorbox{lightgray}{taŋ³⁵}         & 1     & 1 \\
  East Baheng                     & DAUGHTER & \sil pʰje⁵³                              & 2     & 2 \\\hline
  West Baheng                     & SON      & \sil ʔa³/⁰ + \colorbox{lightgray}{taŋ³⁵} & 3 1   & 3 1 \\
  West Baheng                     & DAUGHTER & \sil ta⁵⁵ + qa³/⁰ + tʰjei⁵³              & 4 5 6 & 4 5 6 \\\hline
  Chuanqiandian                   & SON      & \sil \colorbox{lightgray}{to⁴³}          & 1     & 1\\
  Chuanqiandian                   & DAUGHTER & \sil ⁿtsʰai³³                            & 7     & 7 \\\hline
  Chuanqiandian (Central Guizhou) & SON      & \sil tə²/⁰ + \colorbox{lightgray}{tə̃²⁴}  & 8 1   & 8 1\\
  Chuanqiandian (Central Guizhou) & DAUGHTER & \sil \colorbox{lightgray}{tə̃²⁴} + ⁿpʰe⁴² & 9 2   & 1 2\\\hline
  East Qiandong                   & SON      & \sil \colorbox{lightgray}{tei²⁴}         & 1     & 1 \\
  East Qiandong                   & DAUGHTER & \sil \colorbox{lightgray}{tei²⁴} + pʰa³⁵ & 9 2   & 1 2\\\hline
\endtabular}
  \caption{Terms for ``son'' and ``daughter'' across five Hmong-Mien varieties.}
  \label{tab:son}
\end{table}

Our workflow for automatically identifying these cases of cognacy is a new algorithm for
cross-semantic cognate detection, developed first for the work in the Burmish Etymological
Dictionary project lead by Nathan W. Hill. In this workflow, we start from all aligned cognate sets
in our data, and then systematically compare all alignments with each other. Whenever two
alignments are \emph{compatible}, i.e., they have (1)~at least one morpheme in one language
occurring in both aligned cognate sets, which is (2)~identical, and (3)~no shared morphemes in two
alignments which are \emph{not} identical, we treat them as belonging to one and the same cognate
set. We iterate over all alignments in the data algorithmically, merging the alignments into larger
sets in a greedy fashion, and re-assign cognate sets in the data. 

The results can be easily
inspected with help of the EDICTOR tool, for example, by inspecting cognate set distributions in
the data. When inspecting the cross-semantic cognates, which we label \texttt{CROSSIDS} in our data,
the tool will always show, which cognate sets span more than one concept, and users can directly
filter the data and look at the relevant instances. Among the 64 cognate sets reflected in all
languages in our sample, we find quite a few cross-semantically recurring morphemes, seven in total
(with many more for the whole data). The results are shown in Table \ref{tab:cross}.

\begin{table}[htb]
  \centering
  \tabular{llll}\hline
  Language &
  Concept &
  Form & 
  Morphemes \\\hline\hline
  East Baheng & NOSE & ⁿpjau³¹ & \texttt{NOSE} \\
  East Baheng & NASAL MUCUS & qa³/⁰ + ⁿpjau³¹ & \texttt{qa NOSE} \\\hline
  West Luobuohe & TWO & ʔu³¹ & \texttt{TWO} \\\hline
  West Luobuohe & TWENTY & ʔu³¹ + ʑo³¹ & \texttt{TWO zo} \\\hline
  West Baheng & SON & ʔa³/⁰ + taŋ³⁵ & \texttt{SON} \\
  West Baheng & SON-IN-LAW & taŋ³⁵ + wei³¹ & \texttt{SON wei} \\
  West Baheng & GRANDSON & taŋ³⁵ + seŋ³¹ & \texttt{SON seng} \\\hline
  East Qiandong & SUN &  qʰaŋ³³ + nei²⁴ & \texttt{po SUN} \\
  East Qiandong & DAY (NOT NIGHT) & nei²⁴ & \texttt{SUN} \\\hline
  West Baheng & FAECES (EXCREMENT) &  qa³¹ & \texttt{SHIT} \\
  West Baheng & STOMACH & ʔa³/⁰ + tɕʰi³⁵ + qa³¹ & \texttt{a tci SHIT} \\\hline
  West Qiandong & ANT & kæ̃⁴⁴ + mjɔ²² & \texttt{INSECT mjo} \\
  West Qiandong & EARTHWORM &  kæ̃⁴⁴+tɕuŋ⁴⁴ & \texttt{INSECT tsung} \\\hline
  East Baheng & BIRD & taŋ³⁵ + nuŋ³¹ & \texttt{BIRD-A BIRD-B} \\
  East Baheng & NEST & ʑo¹¹ + taŋ³⁵ + nuŋ³¹ & \texttt{zo BIRD-A BIRD-B} \\\hline
  \endtabular
  \caption{Partial cognates among stable concepts with reflexes in all languages in our test
  datasets. We highlight shared cognates by giving a tentative gloss for them in capital letters in
  the column \emph{Morphemes}.}
  \label{tab:cross}
\end{table}

\begin{center}
  \tabular{|p{14cm}|}\hline
  SUMMARY \\\hline
  \begin{itemize}
    \item For a realistic analysis, we need to identify cognates not only within the same meaning
      slot, but across different concepts, specifically when dealing with languages in which
      compounding and word formation are very productive.
    \item We employ a new method that makes use of a comparison of the alignments in readily
      identified and aligned partial cognate sets to identify those morphemes which recur across
      different concepts in our data.
    \item The results can be inspected with help of the EDICTOR, but not directly, by now, only
      indirectly with help of the browser for cognate sets.
    \item The interpretation of the results cannot be done automatically, but requires expert
      assessment with respect to the morphology of the data under consideration.
  \end{itemize}\\\hline
  \endtabular
\end{center}


\subsection{From cross-semantic cognates to correspondence patterns}

\section{Discussion}

\subsection{Possible improvements}

\begin{itemize}
\item semi-automatic reconstruction
\item clearer integration of automatic and semi-automatic methods in teh workflow
\item better handling of output of the automatic tasks (visualziation, etc.)
\end{itemize}

\subsection{General challenges}

\begin{itemize}
\item Lexical reconstruction: how to reconstruct whole words?
\item Sound change representation of all changes along some phylogeny with sound laws
\end{itemize}

\section{Outlook}




\nocite{List2019a}

\printbibliography
\end{document}
